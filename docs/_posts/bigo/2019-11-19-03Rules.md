---
layout: post
title: "Steps to Derive Time Complexity"
categories: bigo
---

[Index]({{site.baseurl}}{% post_url ./bigo/2019-11-19-00Index %})

We can look at deriving time complexity using the following four rules as well just to make sure we understand how this works. Let us look at the four rules.

Rule 1: Consider Only The Worst-Case Scenario
---------------------------------------------

Always consider the worst-case scenario for an algorithm. Let us take an example to understand this, let us take a simple sort algorithm that will loop through each element in the list to find out if an element is present.

Lets first look at the best case i.e., the element being searched is in the first index.

```csharp
var names = new string[] { "findme", "tom",
       "linda", "jack", "giant", "sam" };
var itemsToFind = "findme";
var stepsTaken = 0;
for (int i = 0; i < names.Length; i++)
{
    stepsTaken++;
    if (names[i].Equals(itemsToFind,
        StringComparison.InvariantCultureIgnoreCase))
    {
        Console.WriteLine($"Found Item at index {i}");
        break;
    }
}
Console.WriteLine($"Steps taken {stepsTaken}");

/*
Output
------
Found Item at index 0
Steps taken 1
*/
```

It only takes 1 step to find the string in the array as you see from the example. You may say the time complexity if the item being searched is in the first place, but Big O looks at the scalability of an algorithm it does not care about what the best case is. Let's look at the same example now the worst case.

```csharp
var names = new string[] { "sam", "tom",
       "linda", "jack", "giant", "findme" };
var itemsToFind = "findme";
var stepsTaken = 0;
for (int i = 0; i < names.Length; i++)
{
    stepsTaken++;
    if (names[i].Equals(itemsToFind,
        StringComparison.InvariantCultureIgnoreCase))
    {
        Console.WriteLine($"Found Item at index {i}");
        break;
    }
}
Console.WriteLine($"Steps taken {stepsTaken}");

/*
Output
------
Found Item at index 5
Steps taken 6
*/
```

It takes 6 steps to find the item if its in the last index of the array i.e., to say O(n) in the above example n = 6. But ideally, we would pass in the array as an argument which essentially means n is a variable hence the worst case for this algorithm is O(n) where n is the input size.

Rule 2: Remove Constants
------------------------

Looking at the same example, let's look at the complexity for each line of the algorithm.

```csharp
var names = new string[] { "sam", "tom",
       "linda", "jack", "giant", "findme" };
// For the above assignment the complexity is O(1)
var itemsToFind = "findme";
// For the above assignment the complexity is O(1)
var stepsTaken = 0;
// For the above assignment the complexity is O(1)
for (int i = 0; i < names.Length; i++)
{
    stepsTaken++;
    // For the above operation the complexity is O(1)
    if (names[i].Equals(itemsToFind,
        StringComparison.InvariantCultureIgnoreCase))
    // For the above operation the complexity is O(1)
    {
        Console.WriteLine($"Found Item at index {i}");
        break;
        // We will ignore these two since it has no effect like O(1) anyways
    }
    // The overall complexity of the step in the
    // for loop is O(1 + 1 + 1)/O(1)+O(1)+O(1)
    // Lets consider that to be O(3) for now
    // The overall complexity of the for loop
    // as a whole is hence n*O(3)/O(n*3)
    // Which is O(3n)
}
Console.WriteLine($"Steps taken {stepsTaken}");
// For the above operation the complexity is O(1)
```

Lets add every thing up so the complexity turns out to be O(1) + O(1) + O(1) (from assignments above for loop) + O(3n) (of the for loop) + O(1) (of the write line).

Simplifying O(1 + 1 + 1 + 3n + 1) = O(4 + 3n). As n grows bigger and bigger the constant 4 and three in the expression will have close to no impact on the overall performance of the algorithm. Hence we ignore all the constants in this case and conclude the complexity to be O(n) for this particular algorithm.

+ Rule 3: Different terms for input
+ Rule 4: Drop non Dominants
